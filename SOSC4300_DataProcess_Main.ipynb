{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOSC 4300 Group Project\n",
    "### @auther Li, Jingchen (20583527)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all datasets\n",
    "The weibo data are extracted using the 2 crawlers:\n",
    "https://github.com/dataabc/weibo-crawler\n",
    "https://github.com/dataabc/weiboSpider/tree/master/weibo_spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import all Weibo data\n",
    "Fandom = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Fandom_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "Diba = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Diba_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "Azhong = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\AZhong_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "Azhong2 = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\AZhong_additional_unique_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "China = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\China_support_group_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "China2 = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\China_support_group_additional-unique_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Weibo data with hashtags removed\n",
    "FandomH = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Fandom_full_hashtag_removed.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "DibaH = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Diba_full_hashtag_removed.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "AzhongH = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Azhong_full_hashtag_removed.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "\n",
    "ChinaH = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\China_support_group_full_hashtag_removed.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "Expe = pd.read_csv(\n",
    "r'C:\\R\\SOSC4300\\seventh_expedition_expedition_data.csv',\n",
    "encoding='UTF8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common retweets data\n",
    "FandomRE = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Fandom_retweet_same_original.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "DibaRE = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Diba_retweet_same_origin.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "AzhongRE = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Azhong_retweet_same_original.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify =  pd.read_csv(\n",
    "r'C:\\R\\SOSC4300\\Expedition_labelled.csv',\n",
    "encoding='UTF8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary method\n",
    "### 1. Segment the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkuseg\n",
    "# initialze the segment function\n",
    "segm = pkuseg.pkuseg(model_name = \"web\")           # the 'web' model is trained on Weibo data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-321df2a0c747>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Fandom['content'][i] = segm.cut(Fandom['content'].astype(str)[i])\n"
     ]
    }
   ],
   "source": [
    "# segment all sentences\n",
    "\n",
    "for i in range(len(Fandom)):\n",
    "    Fandom['content'][i] = segm.cut(Fandom['content'].astype(str)[i])\n",
    "for i in range(len(Diba)):\n",
    "    Diba['content'][i] = segm.cut(Diba['content'].astype(str)[i])\n",
    "for i in range(len(Azhong)):\n",
    "    Azhong['content'][i] = segm.cut(Azhong['content'].astype(str)[i])\n",
    "for i in range(len(China)):\n",
    "    China['content'][i] = segm.cut(China['content'].astype(str)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-73c6a52712b1>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  FandomRE['content'][i] = segm.cut(FandomRE['content'].astype(str)[i])\n",
      "<ipython-input-105-73c6a52712b1>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DibaRE['content'][i] = segm.cut(DibaRE['content'].astype(str)[i])\n",
      "<ipython-input-105-73c6a52712b1>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  AzhongRE['content'][i] = segm.cut(AzhongRE['content'].astype(str)[i])\n"
     ]
    }
   ],
   "source": [
    "# segment all sentences\n",
    "for i in range(len(FandomRE)):\n",
    "    FandomRE['content'][i] = segm.cut(FandomRE['content'].astype(str)[i])\n",
    "for i in range(len(DibaRE)):\n",
    "    DibaRE['content'][i] = segm.cut(DibaRE['content'].astype(str)[i])\n",
    "for i in range(len(AzhongRE)):\n",
    "    AzhongRE['content'][i] = segm.cut(AzhongRE['content'].astype(str)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-73-7ebf5fa5e68d>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  China2['content'][i] = segm.cut(China2['content'].astype(str)[i])\n"
     ]
    }
   ],
   "source": [
    "# segment all sentences\n",
    "for i in range(len(China2)):\n",
    "    China2['content'][i] = segm.cut(China2['content'].astype(str)[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idolization dictionary\n",
    "dict_Fan = {\n",
    "    '阿中哥哥':1,\n",
    "'阿中':1,\n",
    "'世最美':1,\n",
    "'世最可':1,\n",
    "'本命':1,\n",
    "'唯粉':1,\n",
    "'我兔':1,\n",
    "'兔子':1,\n",
    "'私生饭':1,\n",
    "'女友粉':1,\n",
    "'妈妈粉':1,\n",
    "'姐姐粉':1,\n",
    "'应援':1,\n",
    "'走花路':1,\n",
    "'云追星':1,\n",
    "'挽尊':1,\n",
    "'糊穿地心':1,\n",
    "'控评':1,\n",
    "'空瓶':1,\n",
    "'Pick':1,\n",
    "'冲鸭':1,\n",
    "'本命':1,\n",
    "'走花路':1,\n",
    "'确认过眼神':1,\n",
    "'盖章':1,\n",
    "'锦鲤':1,\n",
    "'skr':1,\n",
    "'糊了':1,\n",
    "'喜提':1,\n",
    "'C位':1,\n",
    "'抱走':1,\n",
    "'社会人':1,\n",
    "'安排上了':1,\n",
    "'小透明':1,\n",
    "'皮一下':1,\n",
    "'8102':1,\n",
    "'KY':1,\n",
    "'发糖':1,\n",
    "'沙雕':1,\n",
    "'土味情话':1,\n",
    "'大猪蹄子':1,\n",
    "'彩虹屁':1,\n",
    "'视奸':1,\n",
    "'y1s1':1,\n",
    "'有一说一':1,\n",
    "'ky':1,\n",
    "'tcjj':1,\n",
    "'天朝姐姐':1,\n",
    "'blx':1,\n",
    "'玻璃心':1,\n",
    "'hyq':1,\n",
    "'好友圈':1,\n",
    "'awsl':1,\n",
    "'啊我死了':1,\n",
    "'awzh':1,\n",
    "'爱我中华':1,\n",
    "'xswl':1,\n",
    "'xtms':1,\n",
    "'zqsg':1,\n",
    "'jjyy':1,\n",
    "'dbq':1,\n",
    "'xfxy':1,\n",
    "'wlsw':1,\n",
    "'zgbr':1,\n",
    "'mdzz':1,\n",
    "'nmsl':1,\n",
    "'nbcs':1,\n",
    "'rnb':1,\n",
    "'反黑':1,\n",
    "'超话':1,\n",
    "'饭圈女孩':1,\n",
    "'守护':1,\n",
    "'站站':1,\n",
    "'追剧':1,\n",
    "'港剧':1,\n",
    "'后援':1,\n",
    "'爱豆':1,\n",
    "'追星':1,\n",
    "'girl':1,\n",
    "'打卡':1,\n",
    "'抓虫':1,\n",
    "'814':1,\n",
    "'0814':1,\n",
    "'～':1,\n",
    "'守护':1,\n",
    "'帝帝':1,\n",
    "'哇哈哈哈哈':1,\n",
    "'🇭🇰':1,\n",
    "'文宣':1,\n",
    "'粉丝':1,\n",
    "'米果':1,\n",
    "'🇨🇳':1,\n",
    "'🇺🇸':1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idolization dictionary\n",
    "dict_Di = {\n",
    "   '朕':-1,\n",
    "'帝':-1,\n",
    "'帝国':-1,\n",
    "'权威':-1,\n",
    "'权力':-1,\n",
    "'爸爸':-1,\n",
    "'爹':-1,\n",
    "\n",
    "'七子之歌':-1,\n",
    "'儿子':-1,\n",
    "'孙子':-1,\n",
    "\n",
    "'爷爷':-1,\n",
    "'爷':-1,\n",
    "'姑奶奶':-1,\n",
    "'大哥':-1,\n",
    "'天朝':-1,\n",
    "'大国':-1,\n",
    "'五千年':-1,\n",
    "'九百六十万平方公里':-1,\n",
    "'香港市':-1,\n",
    "'香港县' :-1,\n",
    "'草':-1,\n",
    "'港村':-1,\n",
    "'呆湾':-1,\n",
    "'艹':-1,\n",
    "'脑残':-1,\n",
    "'霸权':-1,\n",
    "'土澳':-1,\n",
    "'泡菜宗主国':-1,\n",
    "'耗子':-1,\n",
    "'汉奸':-1,\n",
    "'白痴':-1,\n",
    "'轮子':-1,\n",
    "'川皇':-1,\n",
    "'曱甴':-1,\n",
    "'废青':-1,\n",
    "'黄尸':-1,\n",
    "'帝':-1,\n",
    "'港毒':-1,\n",
    "'大赏':-1,\n",
    "'公知':-1,\n",
    "'女粉':-1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define comments_score() function to caculate Idolization/ Hegemony  score \n",
    "def comments_score(comment,diction):\n",
    "    score = 0\n",
    "    for i in range(len(comment)):\n",
    "        for word in comment[i]:\n",
    "            score += diction.get(word, 0)\n",
    "    \n",
    "    return score\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cacculat the score of each camp\n",
    "DibaF = comments_score(DibaN['content'],dict_Fan)\n",
    "DibaD = comments_score(DibaN['content'],dict_Di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "FandomF = comments_score(Fandom['content'],dict_Fan)\n",
    "FandomD= comments_score(Fandom['content'],dict_Di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "AzhongF = comments_score(Azhong['content'],dict_Fan)\n",
    "AzhongD= comments_score(Azhong['content'],dict_Di)\n",
    "AzhongF2 = comments_score(Azhong2['content'],dict_Fan)\n",
    "AzhongD2= comments_score(Azhong2['content'],dict_Di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChinaF = comments_score(China['content'],dict_Fan)\n",
    "ChinaD= comments_score(China['content'],dict_Di)\n",
    "ChinaF2 = comments_score(China2['content'],dict_Fan)\n",
    "ChinaD2= comments_score(China2['content'],dict_Di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fandom\n",
      "0.4838709677419355\n",
      "-0.09370199692780339\n",
      "Diba\n",
      "0.2032962821004216\n",
      "-0.29451897278650824\n",
      "Azhong\n",
      "1.1636471051595116\n",
      "-0.373769200472627\n",
      "China\n",
      "0.5543850408217014\n",
      "-0.12141164076902818\n"
     ]
    }
   ],
   "source": [
    "# print the result\n",
    "print(\"fandom\")\n",
    "print(FandomF/len(Fandom))\n",
    "print(FandomD/len(Fandom))\n",
    "print(\"Diba\")\n",
    "print(DibaF/len(DibaN))\n",
    "print(DibaD/len(DibaN))\n",
    "print(\"Azhong\")\n",
    "print((AzhongF+AzhongF2)/(len(Azhong)+len(Azhong2)))\n",
    "print((AzhongD+AzhongD2)/(len(Azhong)+len(Azhong2)))\n",
    "print(\"China\")\n",
    "print((ChinaF+ChinaF2)/(len(China)+len(China2)))\n",
    "print((ChinaD+ChinaD2)/(len(China)+len(China2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we improved the function by output the 2 score at once \n",
    "def comments_scores(comment):\n",
    "    scoreF = 0\n",
    "    scoreD = 0\n",
    "    for i in range(len(comment)):\n",
    "        for word in comment[i]:\n",
    "            scoreF += dict_Fan.get(word, 0)\n",
    "            scoreD += dict_Di.get(word, 0)\n",
    "    \n",
    "    return scoreF/len(comment),scoreD/len(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function\n",
    "FandomRE_F,FandomRE_D=comments_scores(FandomRE['content'])\n",
    "DibaRE_F,DibaRE_D  = comments_scores(DibaRE['content'])\n",
    "AzhongRE_F,AzhongRE_D = comments_scores(AzhongRE['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.515625 -0.234375\n",
      "0.28125\n"
     ]
    }
   ],
   "source": [
    "#print the result\n",
    "print(FandomRE_F,FandomRE_D)\n",
    "print(FandomRE_F+FandomRE_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06930693069306931 -0.039603960396039604\n",
      "0.02970297029702971\n"
     ]
    }
   ],
   "source": [
    "print(DibaRE_F,DibaRE_D)\n",
    "print(DibaRE_F+DibaRE_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.263157894736842 -0.05263157894736842\n",
      "2.2105263157894735\n"
     ]
    }
   ],
   "source": [
    "print(AzhongRE_F,AzhongRE_D)\n",
    "print(AzhongRE_F+AzhongRE_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sentiment analysis \n",
    "### 1. XGBoost classifier \n",
    "\n",
    "#### 1.1  Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to transform the array result of segementation back into sentences \n",
    "def seg(comment):\n",
    "    s= ''\n",
    "    for j in comment:\n",
    "        s += j\n",
    "        s += ' '\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-b26fe77fc8e5>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Fandom['content'][i] = seg(Fandom['content'][i])\n"
     ]
    }
   ],
   "source": [
    "# apply the function\n",
    "for i in range(len(Fandom)):\n",
    "    Fandom['content'][i] = seg(Fandom['content'][i])\n",
    "for i in range(len(Diba)):\n",
    "    Diba['content'][i] = seg(Diba['content'][i])\n",
    "for i in range(len(Azhong)):\n",
    "    Azhong['content'][i] = seg(Azhong['content'][i])\n",
    "for i in range(len(China)):\n",
    "    China['content'][i] = seg(China['content'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-92-65b1d8b7143a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DibaH['content'][i] = segm.cut(DibaH['content'].astype(str)[i])\n",
      "<ipython-input-92-65b1d8b7143a>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DibaH['content'][i] = seg(DibaH['content'][i])\n",
      "<ipython-input-92-65b1d8b7143a>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  AzhongH['content'][i] = segm.cut(AzhongH['content'].astype(str)[i])\n",
      "<ipython-input-92-65b1d8b7143a>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  AzhongH['content'][i] = seg(AzhongH['content'][i])\n",
      "<ipython-input-92-65b1d8b7143a>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ChinaH['content'][i] = segm.cut(ChinaH['content'].astype(str)[i])\n",
      "<ipython-input-92-65b1d8b7143a>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ChinaH['content'][i] = seg(ChinaH['content'][i])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(FandomH)):\n",
    "    FandomH['content'][i] = segm.cut(FandomH['content'].astype(str)[i])\n",
    "    FandomH['content'][i] = seg(FandomH['content'][i])\n",
    "for i in range(len(DibaH)):\n",
    "    DibaH['content'][i] = segm.cut(DibaH['content'].astype(str)[i])\n",
    "    DibaH['content'][i] = seg(DibaH['content'][i])\n",
    "for i in range(len(AzhongH)):\n",
    "    AzhongH['content'][i] = segm.cut(AzhongH['content'].astype(str)[i])\n",
    "    AzhongH['content'][i] = seg(AzhongH['content'][i])\n",
    "for i in range(len(ChinaH)):\n",
    "    ChinaH['content'][i] = segm.cut(ChinaH['content'].astype(str)[i])\n",
    "    ChinaH['content'][i] = seg(ChinaH['content'][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 label the data based on the group (or Camp)\n",
    "we try two ways of labeling to see which one yields a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "FandomH['label']=1\n",
    "AzhongH['label']=2\n",
    "ChinaH['label']=3\n",
    "DibaH['label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fandom['label']=1\n",
    "Azhong['label']=1\n",
    "China['label']=1\n",
    "Diba['label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group all records together for training and validation\n",
    "total = Diba.append(Fandom).append(Azhong).append(China)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the binary classification into Idolization/ Hegemony Camp\n",
    "totalH = DibaH.append(FandomH).append(AzhongH).append(ChinaH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the classification into the four identified groups\n",
    "totalH4 = DibaH.append(FandomH).append(AzhongH).append(ChinaH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair-wise group the Diba group with the other three groups\n",
    "DiFan=DibaH.append(FandomH)\n",
    "DiAzhong = DibaH.append(AzhongH)\n",
    "DiChina = DibaH.append(ChinaH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-ebd3b8b277db>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Expe['message'][i] = segm.cut(Expe['message'].astype(str)[i])\n",
      "<ipython-input-100-ebd3b8b277db>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Expe['message'][i] = seg(Expe['message'][i])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                        选择 火烧 白金汉宫 ， 报圆 明园 被 烧 之 仇 ！ \n",
       "1    如果 中国 民主 了 ， 不 知道 中共 会 不 会 下台 ， 可 圆明园 之 仇 必报 ！...\n",
       "2    以前 以为 bbc 是 中立 媒体 ， 原来 bbc 政治 立场 是 那么 的 偏激 ！ 尤...\n",
       "3                                         民主 何時 到 來 ？ \n",
       "4                 感謝 中華 民國 總統 仗義 執言 ， 香港 必勝 ， 共匪 必亡 。 \n",
       "5    陳詠 傑 “ 共匪 ” 要 能 灭亡 的话 早在 蒋介石 时期 就 没有 了 ！ 你 还是 ...\n",
       "6    當初 要不是 契哥 蘇聯 給 你 狗糧 ， 你 班 契弟 早就 死 全族 了 ！ 現 在 共...\n",
       "7                 陳詠 傑 这话 也 就 你们 蟑螂 和 蛙蛙信 ， 抱团 取暖 吧 。 \n",
       "8    陳詠 傑 用错 语言 了 ， 你 的 母语 是 字母 ， 难怪 说 中文 就 脏话 连篇 ，...\n",
       "9    共匪 5毛 你 祖公 叫 列寧 親爹 叫 斯大林 都 是 俄羅斯 鬼子 不是 中國人 ， 死...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one way is to first combine the data \n",
    "for i in range(len(Expe)):\n",
    "    Expe['message'][i] = segm.cut(Expe['message'].astype(str)[i])\n",
    "    Expe['message'][i] = seg(Expe['message'][i])\n",
    "Expe.message.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Training the model\n",
    "We will train 6 models:\n",
    "1. Using binary classification label of Camps, and original posts\n",
    "2. Using binary classification label of Camps, and posts with hashtags removed (hashtags are mainly the name of the weibo account)\n",
    "3. Using classification label of the 4 weibo groups, and posts with hashtags removed \n",
    "\n",
    "4/5/6. Using Diba group against the other 3 groups respectively,and posts with hashtags removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18404,)\n",
      "(2045,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(total.content, total.label, \n",
    "                                                  stratify=total.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21849,)\n",
      "(2428,)\n"
     ]
    }
   ],
   "source": [
    "xHtrain, xHvalid, yHtrain, yHvalid = train_test_split(totalH.content, totalH.label, \n",
    "                                                  stratify=totalH.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (xHtrain.shape)\n",
    "print (xHvalid.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21849,)\n",
      "(2428,)\n"
     ]
    }
   ],
   "source": [
    "xH4train, xH4valid, yH4train, yH4valid = train_test_split(totalH4.content, totalH4.label, \n",
    "                                                  stratify=totalH4.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (xH4train.shape)\n",
    "print (xH4valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14369,)\n",
      "(1597,)\n"
     ]
    }
   ],
   "source": [
    "Fanxtrain, Fanxvalid, Fanytrain, Fanyvalid = train_test_split(DiFan.content, DiFan.label, \n",
    "                                                  stratify=DiFan.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (Fanxtrain.shape)\n",
    "print (Fanxvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16088,)\n",
      "(1788,)\n"
     ]
    }
   ],
   "source": [
    "Azxtrain, Azxvalid, Azytrain, Azyvalid = train_test_split(DiAzhong.content, DiAzhong.label, \n",
    "                                                  stratify=DiAzhong.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (Azxtrain.shape)\n",
    "print (Azyvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15443,)\n",
      "(1716,)\n"
     ]
    }
   ],
   "source": [
    "Chxtrain, Chxvalid, Chytrain, Chyvalid = train_test_split(DiChina.content, DiChina.label, \n",
    "                                                  stratify=DiChina.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (Chxtrain.shape)\n",
    "print (Chyvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20449, 52325)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether the CountVectorizer function work on the data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(list(xtrain) + list(xvalid))\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20449, 52325)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether the TfidfTransformer function work on the data\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the XGBoost Classification model \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range = (1,1))),\n",
    "                   ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "                    ('clf', XGBClassifier(learning_rate = 0.15, n_estimators  = 400)),])\n",
    "\n",
    "text_clf = text_clf.fit(xtrain, ytrain)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "xgb_prediction = text_clf.predict(xvalid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.96      0.87      0.91      1483\n",
      "      Fandom       0.72      0.91      0.81       562\n",
      "\n",
      "    accuracy                           0.88      2045\n",
      "   macro avg       0.84      0.89      0.86      2045\n",
      "weighted avg       0.90      0.88      0.88      2045\n",
      "\n",
      "AUC score:\n",
      "0.8890965941404722\n"
     ]
    }
   ],
   "source": [
    "# output the classification report to see the performance\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "XGB_output_np = np.asarray(xgb_prediction)\n",
    "label_np = np.asarray(yvalid)\n",
    "\n",
    "print(metrics.classification_report(XGB_output_np, label_np,\n",
    "                                    target_names=['Diba','Fandom']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(xgb_prediction, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.94      0.78      0.85      1593\n",
      "      Fandom       0.69      0.90      0.78       835\n",
      "\n",
      "    accuracy                           0.82      2428\n",
      "   macro avg       0.81      0.84      0.82      2428\n",
      "weighted avg       0.85      0.82      0.83      2428\n",
      "\n",
      "AUC score:\n",
      "0.8408444880483853\n"
     ]
    }
   ],
   "source": [
    "text_clf = text_clf.fit(xHtrain, yHtrain)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "xgbH_prediction = text_clf.predict(xHvalid)\n",
    "XGBH_output_np = np.asarray(xgbH_prediction)\n",
    "labelH_np = np.asarray(yHvalid)\n",
    "\n",
    "print(metrics.classification_report(XGBH_output_np, labelH_np,\n",
    "                                    target_names=['Diba','Fandom']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(xgbH_prediction, yHvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:39:17] WARNING: D:\\Build\\xgboost\\xgboost-1.2.1.git\\src\\learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.97      0.75      0.85      1722\n",
      "      Fandom       0.54      0.84      0.66       167\n",
      "      Azhong       0.55      0.89      0.68       276\n",
      "     ChinaSG       0.61      0.87      0.72       263\n",
      "\n",
      "    accuracy                           0.79      2428\n",
      "   macro avg       0.67      0.84      0.73      2428\n",
      "weighted avg       0.85      0.79      0.80      2428\n",
      "\n",
      "AUC score:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-0dac268b18ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                     target_names=['Diba','Fandom','Azhong','ChinaSG']))\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AUC score:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgbH4_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myH4valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    382\u001b[0m                              \"instead\".format(max_fpr))\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         return _multiclass_roc_auc_score(y_true, y_score, labels,\n\u001b[0;32m    386\u001b[0m                                          multi_class, average, sample_weight)\n",
      "\u001b[1;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "text_clf = text_clf.fit(xH4train, yH4train)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "xgbH4_prediction = text_clf.predict(xH4valid)\n",
    "XGBH4_output_np = np.asarray(xgbH4_prediction)\n",
    "labelH4_np = np.asarray(yH4valid)\n",
    "\n",
    "print(metrics.classification_report(XGBH4_output_np, labelH4_np,\n",
    "                                    target_names=['Diba','Fandom','Azhong','ChinaSG']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(xgbH4_prediction, yH4valid)) ## AUC is not for multi-group classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.98      0.92      0.95      1431\n",
      " FandomGirls       0.55      0.86      0.67       166\n",
      "\n",
      "    accuracy                           0.91      1597\n",
      "   macro avg       0.77      0.89      0.81      1597\n",
      "weighted avg       0.94      0.91      0.92      1597\n",
      "\n",
      "AUC score:\n",
      "0.8898423884216111\n"
     ]
    }
   ],
   "source": [
    "text_clf = text_clf.fit(Fanxtrain, Fanytrain)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "Fan_xgb_prediction = text_clf.predict(Fanxvalid)\n",
    "Fan_output_np = np.asarray(Fan_xgb_prediction)\n",
    "Fan_label_np = np.asarray(Fanyvalid)\n",
    "\n",
    "print(metrics.classification_report(Fan_output_np, Fan_label_np,\n",
    "                                    target_names=['Diba','FandomGirls']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(Fan_xgb_prediction, Fanyvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.99      0.88      0.93      1492\n",
      "      Azhong       0.62      0.95      0.75       296\n",
      "\n",
      "    accuracy                           0.89      1788\n",
      "   macro avg       0.80      0.92      0.84      1788\n",
      "weighted avg       0.93      0.89      0.90      1788\n",
      "\n",
      "AUC score:\n",
      "0.9153322223027317\n"
     ]
    }
   ],
   "source": [
    "text_clf = text_clf.fit(Azxtrain, Azytrain)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "Az_xgb_prediction = text_clf.predict(Azxvalid)\n",
    "Az_output_np = np.asarray(Az_xgb_prediction)\n",
    "Az_label_np = np.asarray(Azyvalid)\n",
    "\n",
    "print(metrics.classification_report(Az_output_np, Az_label_np,\n",
    "                                    target_names=['Diba','Azhong']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(Az_xgb_prediction, Azyvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.99      0.90      0.94      1476\n",
      "     ChinaSG       0.60      0.95      0.73       240\n",
      "\n",
      "    accuracy                           0.90      1716\n",
      "   macro avg       0.79      0.92      0.84      1716\n",
      "weighted avg       0.94      0.90      0.91      1716\n",
      "\n",
      "AUC score:\n",
      "0.9210873983739838\n"
     ]
    }
   ],
   "source": [
    "text_clf = text_clf.fit(Chxtrain, Chytrain)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "Ch_xgb_prediction = text_clf.predict(Chxvalid)\n",
    "Ch_output_np = np.asarray(Ch_xgb_prediction)\n",
    "Ch_label_np = np.asarray(Chyvalid)\n",
    "\n",
    "print(metrics.classification_report(Ch_output_np, Ch_label_np,\n",
    "                                    target_names=['Diba','ChinaSG']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(Ch_xgb_prediction, Chyvalid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 build the model and predict the FaceBook Data\n",
    "We use the model trained by weibo data with hashtags removed,although it results in higher f1 score, because it's unlikely the face book data will contain these hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_all = text_clf.fit(totalH.content, totalH.label)\n",
    "expe_prediction = text_clf_all.predict(Expe.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis (Data Processing only)\n",
    "The t-test and regression are done seperatedly in R.\n",
    "In this part we do \n",
    "1. Join the prediction of camp, sentiment analysis result, and simplified Chinese detection result into the original data and save it.\n",
    "2. For each 3-level comment record, join the data of its parent (2-level comments) to it.And only keep the parent that uses simplified Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the prediction result to the original data \n",
    "Expe['prediction']=expe_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>level</th>\n",
       "      <th>object_type</th>\n",
       "      <th>query_status</th>\n",
       "      <th>object_id</th>\n",
       "      <th>message</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>like_count</th>\n",
       "      <th>...</th>\n",
       "      <th>attachments.data.0.description</th>\n",
       "      <th>attachment.type</th>\n",
       "      <th>group</th>\n",
       "      <th>prediction</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_key</th>\n",
       "      <th>positive_probs</th>\n",
       "      <th>negative_probs</th>\n",
       "      <th>simplified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243203</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637201198762</td>\n",
       "      <td>选择 火烧 白金汉宫 ， 报圆 明园 被 烧 之 仇 ！</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:25:11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>选择火烧白金汉宫，报圆明园被烧之仇！</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6947</td>\n",
       "      <td>0.3053</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>243204</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637240313762</td>\n",
       "      <td>如果 中国 民主 了 ， 不 知道 中共 会 不 会 下台 ， 可 圆明园 之 仇 必报 ！...</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:46:48</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>如果中国民主了，不知道中共会不会下台，可圆明园之仇必报！台湾必定武统谢谢！这就是中国民意！</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>243206</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637244948762</td>\n",
       "      <td>以前 以为 bbc 是 中立 媒体 ， 原来 bbc 政治 立场 是 那么 的 偏激 ！ 尤...</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:49:48</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>以前以为bbc是中立媒体，原来bbc政治立场是那么的偏激！尤其昨天香港警察开枪片段竟然剪掉暴...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243221</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637231068762</td>\n",
       "      <td>民主 何時 到 來 ？</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:40:28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>民主何時到來？</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.9119</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>243224</td>\n",
       "      <td>206157</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157635548868762_10157635664488762</td>\n",
       "      <td>感謝 中華 民國 總統 仗義 執言 ， 香港 必勝 ， 共匪 必亡 。</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>11:51:27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>感謝中華民國總統仗義執言， 香港必勝，共匪必亡。</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9440</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  parent_id  level object_type   query_status  \\\n",
       "0  243203     206156      2        data  fetched (200)   \n",
       "1  243204     206156      2        data  fetched (200)   \n",
       "2  243206     206156      2        data  fetched (200)   \n",
       "3  243221     206156      2        data  fetched (200)   \n",
       "4  243224     206157      2        data  fetched (200)   \n",
       "\n",
       "                             object_id  \\\n",
       "0  10157637152528762_10157637201198762   \n",
       "1  10157637152528762_10157637240313762   \n",
       "2  10157637152528762_10157637244948762   \n",
       "3  10157637152528762_10157637231068762   \n",
       "4  10157635548868762_10157635664488762   \n",
       "\n",
       "                                             message        date      time  \\\n",
       "0                      选择 火烧 白金汉宫 ， 报圆 明园 被 烧 之 仇 ！   2019/10/01  23:25:11   \n",
       "1  如果 中国 民主 了 ， 不 知道 中共 会 不 会 下台 ， 可 圆明园 之 仇 必报 ！...  2019/10/01  23:46:48   \n",
       "2  以前 以为 bbc 是 中立 媒体 ， 原来 bbc 政治 立场 是 那么 的 偏激 ！ 尤...  2019/10/01  23:49:48   \n",
       "3                                       民主 何時 到 來 ？   2019/10/01  23:40:28   \n",
       "4               感謝 中華 民國 總統 仗義 執言 ， 香港 必勝 ， 共匪 必亡 。   2019/10/01  11:51:27   \n",
       "\n",
       "   like_count  ...  attachments.data.0.description  attachment.type group  \\\n",
       "0         7.0  ...                             NaN              NaN   bbc   \n",
       "1        13.0  ...                             NaN              NaN   bbc   \n",
       "2         6.0  ...                             NaN              NaN   bbc   \n",
       "3         0.0  ...                             NaN              NaN   bbc   \n",
       "4         2.0  ...                             NaN              NaN   bbc   \n",
       "\n",
       "  prediction                                               text  \\\n",
       "0          0                                 选择火烧白金汉宫，报圆明园被烧之仇！   \n",
       "1          0      如果中国民主了，不知道中共会不会下台，可圆明园之仇必报！台湾必定武统谢谢！这就是中国民意！   \n",
       "2          0  以前以为bbc是中立媒体，原来bbc政治立场是那么的偏激！尤其昨天香港警察开枪片段竟然剪掉暴...   \n",
       "3          0                                            民主何時到來？   \n",
       "4          0                           感謝中華民國總統仗義執言， 香港必勝，共匪必亡。   \n",
       "\n",
       "  sentiment_label  sentiment_key positive_probs  negative_probs  simplified  \n",
       "0               1       positive         0.6947          0.3053        True  \n",
       "1               0       negative         0.1343          0.8657        True  \n",
       "2               0       negative         0.0162          0.9838        True  \n",
       "3               0       negative         0.0881          0.9119       False  \n",
       "4               1       positive         0.9440          0.0560       False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Expe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentiment data generated in another file \n",
    "Expe_sentiment = pd.read_csv(\n",
    "r'C:\\R\\SOSC4300\\sentiment930.csv',\n",
    "encoding='UTF8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also check snownlp package, but the result is not as good as Baidu Senta\n",
    "from snownlp import SnowNLP\n",
    "s =  SnowNLP(Fandom['content'].astype(str)[2599])\n",
    "s.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the simplified Chinese deection result \n",
    "Expe = Expe.join(simplify['simplified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>level</th>\n",
       "      <th>object_type</th>\n",
       "      <th>query_status</th>\n",
       "      <th>object_id</th>\n",
       "      <th>message</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>like_count</th>\n",
       "      <th>...</th>\n",
       "      <th>attachments.data.0.description</th>\n",
       "      <th>attachment.type</th>\n",
       "      <th>group</th>\n",
       "      <th>prediction</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_key</th>\n",
       "      <th>positive_probs</th>\n",
       "      <th>negative_probs</th>\n",
       "      <th>simplified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243203</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637201198762</td>\n",
       "      <td>选择 火烧 白金汉宫 ， 报圆 明园 被 烧 之 仇 ！</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:25:11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>选择火烧白金汉宫，报圆明园被烧之仇！</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6947</td>\n",
       "      <td>0.3053</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>243204</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637240313762</td>\n",
       "      <td>如果 中国 民主 了 ， 不 知道 中共 会 不 会 下台 ， 可 圆明园 之 仇 必报 ！...</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:46:48</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>如果中国民主了，不知道中共会不会下台，可圆明园之仇必报！台湾必定武统谢谢！这就是中国民意！</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  parent_id  level object_type   query_status  \\\n",
       "0  243203     206156      2        data  fetched (200)   \n",
       "1  243204     206156      2        data  fetched (200)   \n",
       "\n",
       "                             object_id  \\\n",
       "0  10157637152528762_10157637201198762   \n",
       "1  10157637152528762_10157637240313762   \n",
       "\n",
       "                                             message        date      time  \\\n",
       "0                      选择 火烧 白金汉宫 ， 报圆 明园 被 烧 之 仇 ！   2019/10/01  23:25:11   \n",
       "1  如果 中国 民主 了 ， 不 知道 中共 会 不 会 下台 ， 可 圆明园 之 仇 必报 ！...  2019/10/01  23:46:48   \n",
       "\n",
       "   like_count  ...  attachments.data.0.description  attachment.type group  \\\n",
       "0         7.0  ...                             NaN              NaN   bbc   \n",
       "1        13.0  ...                             NaN              NaN   bbc   \n",
       "\n",
       "  prediction                                           text sentiment_label  \\\n",
       "0          0                             选择火烧白金汉宫，报圆明园被烧之仇！               1   \n",
       "1          0  如果中国民主了，不知道中共会不会下台，可圆明园之仇必报！台湾必定武统谢谢！这就是中国民意！               0   \n",
       "\n",
       "   sentiment_key positive_probs  negative_probs  simplified  \n",
       "0       positive         0.6947          0.3053        True  \n",
       "1       negative         0.1343          0.8657        True  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Expe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed data\n",
    "Expe.to_csv('seventh_expedition_expedition_data_prediction_sentiment.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4123, 21)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide level 2 and level 3 comments\n",
    "Expe_level3= Expe[Expe['level']==3 ]\n",
    "Expe_level3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12646, 21)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Expe_level2= Expe[Expe['level']==2]\n",
    "Expe_level2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3884, 21)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only simplified level 2 comments\n",
    "Expe_level2= Expe[Expe['simplified']==True]\n",
    "Expe_level2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the parent data to the level 3 comments(its children)\n",
    "Expe_merge = pd.merge(Expe_level3,Expe_level2, left_on = 'parent_id',right_on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(955, 42)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Expe_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe the data\n",
    "Expe_merge.to_csv('seventh_expedition_expedition_data_prediction_sentiment_matched.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. Chen, T., & Guestrin, C. (2016). XGBoost. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. https://doi.org/10.1145/2939672.2939785.  \n",
    "\n",
    "2. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830. \n",
    "\n",
    "3. Ruixuan Luo, Jingjing Xu, Yi Zhang, Xuancheng Ren, Xu Sun. PKUSEG: A Toolkit for Multi-Domain Chinese Word Segmentation. Arxiv. 2019. \n",
    "\n",
    "4. Tian, H., Gao, C., Xiao, X., Liu, H., He, B., Wu, H., Wang, H., & Wu, F. (2020). SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.374. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
