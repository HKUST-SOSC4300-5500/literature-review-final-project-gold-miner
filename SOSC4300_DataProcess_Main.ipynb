{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOSC 4300 Group Project\n",
    "### @auther Li, Jingchen (20583527)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all datasets\n",
    "The weibo data are extracted using the 2 crawlers:\n",
    "https://github.com/dataabc/weibo-crawler\n",
    "https://github.com/dataabc/weiboSpider/tree/master/weibo_spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import all Weibo data\n",
    "Fandom = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Fandom_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "Diba = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Diba_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "Azhong = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\AZhong_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "Azhong2 = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\AZhong_additional_unique_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "China = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\China_support_group_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "China2 = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\China_support_group_additional-unique_excel_formated.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Weibo data with hashtags removed\n",
    "FandomH = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Fandom_full_hashtag_removed.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "DibaH = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Diba_full_hashtag_removed.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "AzhongH = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Azhong_full_hashtag_removed.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "\n",
    "ChinaH = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\China_support_group_full_hashtag_removed.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "Expe = pd.read_csv(\n",
    "r'C:\\R\\SOSC4300\\seventh_expedition_expedition_data.csv',\n",
    "encoding='UTF8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common retweets data\n",
    "FandomRE = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Fandom_retweet_same_original.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "DibaRE = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Diba_retweet_same_origin.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n",
    "AzhongRE = pd.read_excel(\n",
    "r'C:\\R\\SOSC4300\\Azhong_retweet_same_original.xlsx',\n",
    "encoding='UTF8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify =  pd.read_csv(\n",
    "r'C:\\R\\SOSC4300\\Expedition_labelled.csv',\n",
    "encoding='UTF8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary method\n",
    "### 1. Segment the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkuseg\n",
    "# initialze the segment function\n",
    "segm = pkuseg.pkuseg(model_name = \"web\")           # the 'web' model is trained on Weibo data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-321df2a0c747>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Fandom['content'][i] = segm.cut(Fandom['content'].astype(str)[i])\n"
     ]
    }
   ],
   "source": [
    "# segment all sentences\n",
    "\n",
    "for i in range(len(Fandom)):\n",
    "    Fandom['content'][i] = segm.cut(Fandom['content'].astype(str)[i])\n",
    "for i in range(len(Diba)):\n",
    "    Diba['content'][i] = segm.cut(Diba['content'].astype(str)[i])\n",
    "for i in range(len(Azhong)):\n",
    "    Azhong['content'][i] = segm.cut(Azhong['content'].astype(str)[i])\n",
    "for i in range(len(China)):\n",
    "    China['content'][i] = segm.cut(China['content'].astype(str)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-73c6a52712b1>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  FandomRE['content'][i] = segm.cut(FandomRE['content'].astype(str)[i])\n",
      "<ipython-input-105-73c6a52712b1>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DibaRE['content'][i] = segm.cut(DibaRE['content'].astype(str)[i])\n",
      "<ipython-input-105-73c6a52712b1>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  AzhongRE['content'][i] = segm.cut(AzhongRE['content'].astype(str)[i])\n"
     ]
    }
   ],
   "source": [
    "# segment all sentences\n",
    "for i in range(len(FandomRE)):\n",
    "    FandomRE['content'][i] = segm.cut(FandomRE['content'].astype(str)[i])\n",
    "for i in range(len(DibaRE)):\n",
    "    DibaRE['content'][i] = segm.cut(DibaRE['content'].astype(str)[i])\n",
    "for i in range(len(AzhongRE)):\n",
    "    AzhongRE['content'][i] = segm.cut(AzhongRE['content'].astype(str)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-73-7ebf5fa5e68d>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  China2['content'][i] = segm.cut(China2['content'].astype(str)[i])\n"
     ]
    }
   ],
   "source": [
    "# segment all sentences\n",
    "for i in range(len(China2)):\n",
    "    China2['content'][i] = segm.cut(China2['content'].astype(str)[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idolization dictionary\n",
    "dict_Fan = {\n",
    "    'é˜¿ä¸­å“¥å“¥':1,\n",
    "'é˜¿ä¸­':1,\n",
    "'ä¸–æœ€ç¾':1,\n",
    "'ä¸–æœ€å¯':1,\n",
    "'æœ¬å‘½':1,\n",
    "'å”¯ç²‰':1,\n",
    "'æˆ‘å…”':1,\n",
    "'å…”å­':1,\n",
    "'ç§ç”Ÿé¥­':1,\n",
    "'å¥³å‹ç²‰':1,\n",
    "'å¦ˆå¦ˆç²‰':1,\n",
    "'å§å§ç²‰':1,\n",
    "'åº”æ´':1,\n",
    "'èµ°èŠ±è·¯':1,\n",
    "'äº‘è¿½æ˜Ÿ':1,\n",
    "'æŒ½å°Š':1,\n",
    "'ç³Šç©¿åœ°å¿ƒ':1,\n",
    "'æ§è¯„':1,\n",
    "'ç©ºç“¶':1,\n",
    "'Pick':1,\n",
    "'å†²é¸­':1,\n",
    "'æœ¬å‘½':1,\n",
    "'èµ°èŠ±è·¯':1,\n",
    "'ç¡®è®¤è¿‡çœ¼ç¥':1,\n",
    "'ç›–ç« ':1,\n",
    "'é”¦é²¤':1,\n",
    "'skr':1,\n",
    "'ç³Šäº†':1,\n",
    "'å–œæ':1,\n",
    "'Cä½':1,\n",
    "'æŠ±èµ°':1,\n",
    "'ç¤¾ä¼šäºº':1,\n",
    "'å®‰æ’ä¸Šäº†':1,\n",
    "'å°é€æ˜':1,\n",
    "'çš®ä¸€ä¸‹':1,\n",
    "'8102':1,\n",
    "'KY':1,\n",
    "'å‘ç³–':1,\n",
    "'æ²™é›•':1,\n",
    "'åœŸå‘³æƒ…è¯':1,\n",
    "'å¤§çŒªè¹„å­':1,\n",
    "'å½©è™¹å±':1,\n",
    "'è§†å¥¸':1,\n",
    "'y1s1':1,\n",
    "'æœ‰ä¸€è¯´ä¸€':1,\n",
    "'ky':1,\n",
    "'tcjj':1,\n",
    "'å¤©æœå§å§':1,\n",
    "'blx':1,\n",
    "'ç»ç’ƒå¿ƒ':1,\n",
    "'hyq':1,\n",
    "'å¥½å‹åœˆ':1,\n",
    "'awsl':1,\n",
    "'å•Šæˆ‘æ­»äº†':1,\n",
    "'awzh':1,\n",
    "'çˆ±æˆ‘ä¸­å':1,\n",
    "'xswl':1,\n",
    "'xtms':1,\n",
    "'zqsg':1,\n",
    "'jjyy':1,\n",
    "'dbq':1,\n",
    "'xfxy':1,\n",
    "'wlsw':1,\n",
    "'zgbr':1,\n",
    "'mdzz':1,\n",
    "'nmsl':1,\n",
    "'nbcs':1,\n",
    "'rnb':1,\n",
    "'åé»‘':1,\n",
    "'è¶…è¯':1,\n",
    "'é¥­åœˆå¥³å­©':1,\n",
    "'å®ˆæŠ¤':1,\n",
    "'ç«™ç«™':1,\n",
    "'è¿½å‰§':1,\n",
    "'æ¸¯å‰§':1,\n",
    "'åæ´':1,\n",
    "'çˆ±è±†':1,\n",
    "'è¿½æ˜Ÿ':1,\n",
    "'girl':1,\n",
    "'æ‰“å¡':1,\n",
    "'æŠ“è™«':1,\n",
    "'814':1,\n",
    "'0814':1,\n",
    "'ï½':1,\n",
    "'å®ˆæŠ¤':1,\n",
    "'å¸å¸':1,\n",
    "'å“‡å“ˆå“ˆå“ˆå“ˆ':1,\n",
    "'ğŸ‡­ğŸ‡°':1,\n",
    "'æ–‡å®£':1,\n",
    "'ç²‰ä¸':1,\n",
    "'ç±³æœ':1,\n",
    "'ğŸ‡¨ğŸ‡³':1,\n",
    "'ğŸ‡ºğŸ‡¸':1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idolization dictionary\n",
    "dict_Di = {\n",
    "   'æœ•':-1,\n",
    "'å¸':-1,\n",
    "'å¸å›½':-1,\n",
    "'æƒå¨':-1,\n",
    "'æƒåŠ›':-1,\n",
    "'çˆ¸çˆ¸':-1,\n",
    "'çˆ¹':-1,\n",
    "\n",
    "'ä¸ƒå­ä¹‹æ­Œ':-1,\n",
    "'å„¿å­':-1,\n",
    "'å­™å­':-1,\n",
    "\n",
    "'çˆ·çˆ·':-1,\n",
    "'çˆ·':-1,\n",
    "'å§‘å¥¶å¥¶':-1,\n",
    "'å¤§å“¥':-1,\n",
    "'å¤©æœ':-1,\n",
    "'å¤§å›½':-1,\n",
    "'äº”åƒå¹´':-1,\n",
    "'ä¹ç™¾å…­åä¸‡å¹³æ–¹å…¬é‡Œ':-1,\n",
    "'é¦™æ¸¯å¸‚':-1,\n",
    "'é¦™æ¸¯å¿' :-1,\n",
    "'è‰':-1,\n",
    "'æ¸¯æ‘':-1,\n",
    "'å‘†æ¹¾':-1,\n",
    "'è‰¹':-1,\n",
    "'è„‘æ®‹':-1,\n",
    "'éœ¸æƒ':-1,\n",
    "'åœŸæ¾³':-1,\n",
    "'æ³¡èœå®—ä¸»å›½':-1,\n",
    "'è€—å­':-1,\n",
    "'æ±‰å¥¸':-1,\n",
    "'ç™½ç—´':-1,\n",
    "'è½®å­':-1,\n",
    "'å·çš‡':-1,\n",
    "'æ›±ç”´':-1,\n",
    "'åºŸé’':-1,\n",
    "'é»„å°¸':-1,\n",
    "'å¸':-1,\n",
    "'æ¸¯æ¯’':-1,\n",
    "'å¤§èµ':-1,\n",
    "'å…¬çŸ¥':-1,\n",
    "'å¥³ç²‰':-1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define comments_score() function to caculate Idolization/ Hegemony  score \n",
    "def comments_score(comment,diction):\n",
    "    score = 0\n",
    "    for i in range(len(comment)):\n",
    "        for word in comment[i]:\n",
    "            score += diction.get(word, 0)\n",
    "    \n",
    "    return score\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cacculat the score of each camp\n",
    "DibaF = comments_score(DibaN['content'],dict_Fan)\n",
    "DibaD = comments_score(DibaN['content'],dict_Di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "FandomF = comments_score(Fandom['content'],dict_Fan)\n",
    "FandomD= comments_score(Fandom['content'],dict_Di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "AzhongF = comments_score(Azhong['content'],dict_Fan)\n",
    "AzhongD= comments_score(Azhong['content'],dict_Di)\n",
    "AzhongF2 = comments_score(Azhong2['content'],dict_Fan)\n",
    "AzhongD2= comments_score(Azhong2['content'],dict_Di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChinaF = comments_score(China['content'],dict_Fan)\n",
    "ChinaD= comments_score(China['content'],dict_Di)\n",
    "ChinaF2 = comments_score(China2['content'],dict_Fan)\n",
    "ChinaD2= comments_score(China2['content'],dict_Di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fandom\n",
      "0.4838709677419355\n",
      "-0.09370199692780339\n",
      "Diba\n",
      "0.2032962821004216\n",
      "-0.29451897278650824\n",
      "Azhong\n",
      "1.1636471051595116\n",
      "-0.373769200472627\n",
      "China\n",
      "0.5543850408217014\n",
      "-0.12141164076902818\n"
     ]
    }
   ],
   "source": [
    "# print the result\n",
    "print(\"fandom\")\n",
    "print(FandomF/len(Fandom))\n",
    "print(FandomD/len(Fandom))\n",
    "print(\"Diba\")\n",
    "print(DibaF/len(DibaN))\n",
    "print(DibaD/len(DibaN))\n",
    "print(\"Azhong\")\n",
    "print((AzhongF+AzhongF2)/(len(Azhong)+len(Azhong2)))\n",
    "print((AzhongD+AzhongD2)/(len(Azhong)+len(Azhong2)))\n",
    "print(\"China\")\n",
    "print((ChinaF+ChinaF2)/(len(China)+len(China2)))\n",
    "print((ChinaD+ChinaD2)/(len(China)+len(China2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we improved the function by output the 2 score at once \n",
    "def comments_scores(comment):\n",
    "    scoreF = 0\n",
    "    scoreD = 0\n",
    "    for i in range(len(comment)):\n",
    "        for word in comment[i]:\n",
    "            scoreF += dict_Fan.get(word, 0)\n",
    "            scoreD += dict_Di.get(word, 0)\n",
    "    \n",
    "    return scoreF/len(comment),scoreD/len(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function\n",
    "FandomRE_F,FandomRE_D=comments_scores(FandomRE['content'])\n",
    "DibaRE_F,DibaRE_D  = comments_scores(DibaRE['content'])\n",
    "AzhongRE_F,AzhongRE_D = comments_scores(AzhongRE['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.515625 -0.234375\n",
      "0.28125\n"
     ]
    }
   ],
   "source": [
    "#print the result\n",
    "print(FandomRE_F,FandomRE_D)\n",
    "print(FandomRE_F+FandomRE_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06930693069306931 -0.039603960396039604\n",
      "0.02970297029702971\n"
     ]
    }
   ],
   "source": [
    "print(DibaRE_F,DibaRE_D)\n",
    "print(DibaRE_F+DibaRE_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.263157894736842 -0.05263157894736842\n",
      "2.2105263157894735\n"
     ]
    }
   ],
   "source": [
    "print(AzhongRE_F,AzhongRE_D)\n",
    "print(AzhongRE_F+AzhongRE_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sentiment analysis \n",
    "### 1. XGBoost classifier \n",
    "\n",
    "#### 1.1  Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to transform the array result of segementation back into sentences \n",
    "def seg(comment):\n",
    "    s= ''\n",
    "    for j in comment:\n",
    "        s += j\n",
    "        s += ' '\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-b26fe77fc8e5>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Fandom['content'][i] = seg(Fandom['content'][i])\n"
     ]
    }
   ],
   "source": [
    "# apply the function\n",
    "for i in range(len(Fandom)):\n",
    "    Fandom['content'][i] = seg(Fandom['content'][i])\n",
    "for i in range(len(Diba)):\n",
    "    Diba['content'][i] = seg(Diba['content'][i])\n",
    "for i in range(len(Azhong)):\n",
    "    Azhong['content'][i] = seg(Azhong['content'][i])\n",
    "for i in range(len(China)):\n",
    "    China['content'][i] = seg(China['content'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-92-65b1d8b7143a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DibaH['content'][i] = segm.cut(DibaH['content'].astype(str)[i])\n",
      "<ipython-input-92-65b1d8b7143a>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DibaH['content'][i] = seg(DibaH['content'][i])\n",
      "<ipython-input-92-65b1d8b7143a>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  AzhongH['content'][i] = segm.cut(AzhongH['content'].astype(str)[i])\n",
      "<ipython-input-92-65b1d8b7143a>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  AzhongH['content'][i] = seg(AzhongH['content'][i])\n",
      "<ipython-input-92-65b1d8b7143a>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ChinaH['content'][i] = segm.cut(ChinaH['content'].astype(str)[i])\n",
      "<ipython-input-92-65b1d8b7143a>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ChinaH['content'][i] = seg(ChinaH['content'][i])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(FandomH)):\n",
    "    FandomH['content'][i] = segm.cut(FandomH['content'].astype(str)[i])\n",
    "    FandomH['content'][i] = seg(FandomH['content'][i])\n",
    "for i in range(len(DibaH)):\n",
    "    DibaH['content'][i] = segm.cut(DibaH['content'].astype(str)[i])\n",
    "    DibaH['content'][i] = seg(DibaH['content'][i])\n",
    "for i in range(len(AzhongH)):\n",
    "    AzhongH['content'][i] = segm.cut(AzhongH['content'].astype(str)[i])\n",
    "    AzhongH['content'][i] = seg(AzhongH['content'][i])\n",
    "for i in range(len(ChinaH)):\n",
    "    ChinaH['content'][i] = segm.cut(ChinaH['content'].astype(str)[i])\n",
    "    ChinaH['content'][i] = seg(ChinaH['content'][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 label the data based on the group (or Camp)\n",
    "we try two ways of labeling to see which one yields a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "FandomH['label']=1\n",
    "AzhongH['label']=2\n",
    "ChinaH['label']=3\n",
    "DibaH['label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fandom['label']=1\n",
    "Azhong['label']=1\n",
    "China['label']=1\n",
    "Diba['label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group all records together for training and validation\n",
    "total = Diba.append(Fandom).append(Azhong).append(China)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the binary classification into Idolization/ Hegemony Camp\n",
    "totalH = DibaH.append(FandomH).append(AzhongH).append(ChinaH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the classification into the four identified groups\n",
    "totalH4 = DibaH.append(FandomH).append(AzhongH).append(ChinaH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair-wise group the Diba group with the other three groups\n",
    "DiFan=DibaH.append(FandomH)\n",
    "DiAzhong = DibaH.append(AzhongH)\n",
    "DiChina = DibaH.append(ChinaH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-ebd3b8b277db>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Expe['message'][i] = segm.cut(Expe['message'].astype(str)[i])\n",
      "<ipython-input-100-ebd3b8b277db>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Expe['message'][i] = seg(Expe['message'][i])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                        é€‰æ‹© ç«çƒ§ ç™½é‡‘æ±‰å®« ï¼Œ æŠ¥åœ† æ˜å›­ è¢« çƒ§ ä¹‹ ä»‡ ï¼ \n",
       "1    å¦‚æœ ä¸­å›½ æ°‘ä¸» äº† ï¼Œ ä¸ çŸ¥é“ ä¸­å…± ä¼š ä¸ ä¼š ä¸‹å° ï¼Œ å¯ åœ†æ˜å›­ ä¹‹ ä»‡ å¿…æŠ¥ ï¼...\n",
       "2    ä»¥å‰ ä»¥ä¸º bbc æ˜¯ ä¸­ç«‹ åª’ä½“ ï¼Œ åŸæ¥ bbc æ”¿æ²» ç«‹åœº æ˜¯ é‚£ä¹ˆ çš„ åæ¿€ ï¼ å°¤...\n",
       "3                                         æ°‘ä¸» ä½•æ™‚ åˆ° ä¾† ï¼Ÿ \n",
       "4                 æ„Ÿè¬ ä¸­è¯ æ°‘åœ‹ ç¸½çµ± ä»—ç¾© åŸ·è¨€ ï¼Œ é¦™æ¸¯ å¿…å‹ ï¼Œ å…±åŒª å¿…äº¡ ã€‚ \n",
       "5    é™³è©  å‚‘ â€œ å…±åŒª â€ è¦ èƒ½ ç­äº¡ çš„è¯ æ—©åœ¨ è’‹ä»‹çŸ³ æ—¶æœŸ å°± æ²¡æœ‰ äº† ï¼ ä½  è¿˜æ˜¯ ...\n",
       "6    ç•¶åˆ è¦ä¸æ˜¯ å¥‘å“¥ è˜‡è¯ çµ¦ ä½  ç‹—ç³§ ï¼Œ ä½  ç­ å¥‘å¼Ÿ æ—©å°± æ­» å…¨æ— äº† ï¼ ç¾ åœ¨ å…±...\n",
       "7                 é™³è©  å‚‘ è¿™è¯ ä¹Ÿ å°± ä½ ä»¬ èŸ‘è‚ å’Œ è›™è›™ä¿¡ ï¼Œ æŠ±å›¢ å–æš– å§ ã€‚ \n",
       "8    é™³è©  å‚‘ ç”¨é”™ è¯­è¨€ äº† ï¼Œ ä½  çš„ æ¯è¯­ æ˜¯ å­—æ¯ ï¼Œ éš¾æ€ª è¯´ ä¸­æ–‡ å°± è„è¯ è¿ç¯‡ ï¼Œ...\n",
       "9    å…±åŒª 5æ¯› ä½  ç¥–å…¬ å« åˆ—å¯§ è¦ªçˆ¹ å« æ–¯å¤§æ— éƒ½ æ˜¯ ä¿„ç¾…æ–¯ é¬¼å­ ä¸æ˜¯ ä¸­åœ‹äºº ï¼Œ æ­»...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one way is to first combine the data \n",
    "for i in range(len(Expe)):\n",
    "    Expe['message'][i] = segm.cut(Expe['message'].astype(str)[i])\n",
    "    Expe['message'][i] = seg(Expe['message'][i])\n",
    "Expe.message.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Training the model\n",
    "We will train 6 models:\n",
    "1. Using binary classification label of Camps, and original posts\n",
    "2. Using binary classification label of Camps, and posts with hashtags removed (hashtags are mainly the name of the weibo account)\n",
    "3. Using classification label of the 4 weibo groups, and posts with hashtags removed \n",
    "\n",
    "4/5/6. Using Diba group against the other 3 groups respectively,and posts with hashtags removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18404,)\n",
      "(2045,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(total.content, total.label, \n",
    "                                                  stratify=total.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21849,)\n",
      "(2428,)\n"
     ]
    }
   ],
   "source": [
    "xHtrain, xHvalid, yHtrain, yHvalid = train_test_split(totalH.content, totalH.label, \n",
    "                                                  stratify=totalH.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (xHtrain.shape)\n",
    "print (xHvalid.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21849,)\n",
      "(2428,)\n"
     ]
    }
   ],
   "source": [
    "xH4train, xH4valid, yH4train, yH4valid = train_test_split(totalH4.content, totalH4.label, \n",
    "                                                  stratify=totalH4.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (xH4train.shape)\n",
    "print (xH4valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14369,)\n",
      "(1597,)\n"
     ]
    }
   ],
   "source": [
    "Fanxtrain, Fanxvalid, Fanytrain, Fanyvalid = train_test_split(DiFan.content, DiFan.label, \n",
    "                                                  stratify=DiFan.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (Fanxtrain.shape)\n",
    "print (Fanxvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16088,)\n",
      "(1788,)\n"
     ]
    }
   ],
   "source": [
    "Azxtrain, Azxvalid, Azytrain, Azyvalid = train_test_split(DiAzhong.content, DiAzhong.label, \n",
    "                                                  stratify=DiAzhong.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (Azxtrain.shape)\n",
    "print (Azyvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15443,)\n",
      "(1716,)\n"
     ]
    }
   ],
   "source": [
    "Chxtrain, Chxvalid, Chytrain, Chyvalid = train_test_split(DiChina.content, DiChina.label, \n",
    "                                                  stratify=DiChina.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (Chxtrain.shape)\n",
    "print (Chyvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20449, 52325)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether the CountVectorizer function work on the data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(list(xtrain) + list(xvalid))\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20449, 52325)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether the TfidfTransformer function work on the data\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the XGBoost Classification model \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range = (1,1))),\n",
    "                   ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "                    ('clf', XGBClassifier(learning_rate = 0.15, n_estimators  = 400)),])\n",
    "\n",
    "text_clf = text_clf.fit(xtrain, ytrain)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "xgb_prediction = text_clf.predict(xvalid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.96      0.87      0.91      1483\n",
      "      Fandom       0.72      0.91      0.81       562\n",
      "\n",
      "    accuracy                           0.88      2045\n",
      "   macro avg       0.84      0.89      0.86      2045\n",
      "weighted avg       0.90      0.88      0.88      2045\n",
      "\n",
      "AUC score:\n",
      "0.8890965941404722\n"
     ]
    }
   ],
   "source": [
    "# output the classification report to see the performance\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "XGB_output_np = np.asarray(xgb_prediction)\n",
    "label_np = np.asarray(yvalid)\n",
    "\n",
    "print(metrics.classification_report(XGB_output_np, label_np,\n",
    "                                    target_names=['Diba','Fandom']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(xgb_prediction, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.94      0.78      0.85      1593\n",
      "      Fandom       0.69      0.90      0.78       835\n",
      "\n",
      "    accuracy                           0.82      2428\n",
      "   macro avg       0.81      0.84      0.82      2428\n",
      "weighted avg       0.85      0.82      0.83      2428\n",
      "\n",
      "AUC score:\n",
      "0.8408444880483853\n"
     ]
    }
   ],
   "source": [
    "text_clf = text_clf.fit(xHtrain, yHtrain)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "xgbH_prediction = text_clf.predict(xHvalid)\n",
    "XGBH_output_np = np.asarray(xgbH_prediction)\n",
    "labelH_np = np.asarray(yHvalid)\n",
    "\n",
    "print(metrics.classification_report(XGBH_output_np, labelH_np,\n",
    "                                    target_names=['Diba','Fandom']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(xgbH_prediction, yHvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:39:17] WARNING: D:\\Build\\xgboost\\xgboost-1.2.1.git\\src\\learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.97      0.75      0.85      1722\n",
      "      Fandom       0.54      0.84      0.66       167\n",
      "      Azhong       0.55      0.89      0.68       276\n",
      "     ChinaSG       0.61      0.87      0.72       263\n",
      "\n",
      "    accuracy                           0.79      2428\n",
      "   macro avg       0.67      0.84      0.73      2428\n",
      "weighted avg       0.85      0.79      0.80      2428\n",
      "\n",
      "AUC score:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-0dac268b18ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                     target_names=['Diba','Fandom','Azhong','ChinaSG']))\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AUC score:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgbH4_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myH4valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    382\u001b[0m                              \"instead\".format(max_fpr))\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         return _multiclass_roc_auc_score(y_true, y_score, labels,\n\u001b[0;32m    386\u001b[0m                                          multi_class, average, sample_weight)\n",
      "\u001b[1;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "text_clf = text_clf.fit(xH4train, yH4train)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "xgbH4_prediction = text_clf.predict(xH4valid)\n",
    "XGBH4_output_np = np.asarray(xgbH4_prediction)\n",
    "labelH4_np = np.asarray(yH4valid)\n",
    "\n",
    "print(metrics.classification_report(XGBH4_output_np, labelH4_np,\n",
    "                                    target_names=['Diba','Fandom','Azhong','ChinaSG']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(xgbH4_prediction, yH4valid)) ## AUC is not for multi-group classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.98      0.92      0.95      1431\n",
      " FandomGirls       0.55      0.86      0.67       166\n",
      "\n",
      "    accuracy                           0.91      1597\n",
      "   macro avg       0.77      0.89      0.81      1597\n",
      "weighted avg       0.94      0.91      0.92      1597\n",
      "\n",
      "AUC score:\n",
      "0.8898423884216111\n"
     ]
    }
   ],
   "source": [
    "text_clf = text_clf.fit(Fanxtrain, Fanytrain)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "Fan_xgb_prediction = text_clf.predict(Fanxvalid)\n",
    "Fan_output_np = np.asarray(Fan_xgb_prediction)\n",
    "Fan_label_np = np.asarray(Fanyvalid)\n",
    "\n",
    "print(metrics.classification_report(Fan_output_np, Fan_label_np,\n",
    "                                    target_names=['Diba','FandomGirls']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(Fan_xgb_prediction, Fanyvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.99      0.88      0.93      1492\n",
      "      Azhong       0.62      0.95      0.75       296\n",
      "\n",
      "    accuracy                           0.89      1788\n",
      "   macro avg       0.80      0.92      0.84      1788\n",
      "weighted avg       0.93      0.89      0.90      1788\n",
      "\n",
      "AUC score:\n",
      "0.9153322223027317\n"
     ]
    }
   ],
   "source": [
    "text_clf = text_clf.fit(Azxtrain, Azytrain)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "Az_xgb_prediction = text_clf.predict(Azxvalid)\n",
    "Az_output_np = np.asarray(Az_xgb_prediction)\n",
    "Az_label_np = np.asarray(Azyvalid)\n",
    "\n",
    "print(metrics.classification_report(Az_output_np, Az_label_np,\n",
    "                                    target_names=['Diba','Azhong']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(Az_xgb_prediction, Azyvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Diba       0.99      0.90      0.94      1476\n",
      "     ChinaSG       0.60      0.95      0.73       240\n",
      "\n",
      "    accuracy                           0.90      1716\n",
      "   macro avg       0.79      0.92      0.84      1716\n",
      "weighted avg       0.94      0.90      0.91      1716\n",
      "\n",
      "AUC score:\n",
      "0.9210873983739838\n"
     ]
    }
   ],
   "source": [
    "text_clf = text_clf.fit(Chxtrain, Chytrain)\n",
    "\n",
    "# predict the test data and calculate the simplest roc_auc_score\n",
    "Ch_xgb_prediction = text_clf.predict(Chxvalid)\n",
    "Ch_output_np = np.asarray(Ch_xgb_prediction)\n",
    "Ch_label_np = np.asarray(Chyvalid)\n",
    "\n",
    "print(metrics.classification_report(Ch_output_np, Ch_label_np,\n",
    "                                    target_names=['Diba','ChinaSG']))\n",
    "print(\"AUC score:\")\n",
    "print(metrics.roc_auc_score(Ch_xgb_prediction, Chyvalid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 build the model and predict the FaceBook Data\n",
    "We use the model trained by weibo data with hashtags removed,although it results in higher f1 score, because it's unlikely the face book data will contain these hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_all = text_clf.fit(totalH.content, totalH.label)\n",
    "expe_prediction = text_clf_all.predict(Expe.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis (Data Processing only)\n",
    "The t-test and regression are done seperatedly in R.\n",
    "In this part we do \n",
    "1. Join the prediction of camp, sentiment analysis result, and simplified Chinese detection result into the original data and save it.\n",
    "2. For each 3-level comment record, join the data of its parent (2-level comments) to it.And only keep the parent that uses simplified Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the prediction result to the original data \n",
    "Expe['prediction']=expe_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>level</th>\n",
       "      <th>object_type</th>\n",
       "      <th>query_status</th>\n",
       "      <th>object_id</th>\n",
       "      <th>message</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>like_count</th>\n",
       "      <th>...</th>\n",
       "      <th>attachments.data.0.description</th>\n",
       "      <th>attachment.type</th>\n",
       "      <th>group</th>\n",
       "      <th>prediction</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_key</th>\n",
       "      <th>positive_probs</th>\n",
       "      <th>negative_probs</th>\n",
       "      <th>simplified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243203</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637201198762</td>\n",
       "      <td>é€‰æ‹© ç«çƒ§ ç™½é‡‘æ±‰å®« ï¼Œ æŠ¥åœ† æ˜å›­ è¢« çƒ§ ä¹‹ ä»‡ ï¼</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:25:11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>é€‰æ‹©ç«çƒ§ç™½é‡‘æ±‰å®«ï¼ŒæŠ¥åœ†æ˜å›­è¢«çƒ§ä¹‹ä»‡ï¼</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6947</td>\n",
       "      <td>0.3053</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>243204</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637240313762</td>\n",
       "      <td>å¦‚æœ ä¸­å›½ æ°‘ä¸» äº† ï¼Œ ä¸ çŸ¥é“ ä¸­å…± ä¼š ä¸ ä¼š ä¸‹å° ï¼Œ å¯ åœ†æ˜å›­ ä¹‹ ä»‡ å¿…æŠ¥ ï¼...</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:46:48</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>å¦‚æœä¸­å›½æ°‘ä¸»äº†ï¼Œä¸çŸ¥é“ä¸­å…±ä¼šä¸ä¼šä¸‹å°ï¼Œå¯åœ†æ˜å›­ä¹‹ä»‡å¿…æŠ¥ï¼å°æ¹¾å¿…å®šæ­¦ç»Ÿè°¢è°¢ï¼è¿™å°±æ˜¯ä¸­å›½æ°‘æ„ï¼</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>243206</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637244948762</td>\n",
       "      <td>ä»¥å‰ ä»¥ä¸º bbc æ˜¯ ä¸­ç«‹ åª’ä½“ ï¼Œ åŸæ¥ bbc æ”¿æ²» ç«‹åœº æ˜¯ é‚£ä¹ˆ çš„ åæ¿€ ï¼ å°¤...</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:49:48</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>ä»¥å‰ä»¥ä¸ºbbcæ˜¯ä¸­ç«‹åª’ä½“ï¼ŒåŸæ¥bbcæ”¿æ²»ç«‹åœºæ˜¯é‚£ä¹ˆçš„åæ¿€ï¼å°¤å…¶æ˜¨å¤©é¦™æ¸¯è­¦å¯Ÿå¼€æªç‰‡æ®µç«Ÿç„¶å‰ªæ‰æš´...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243221</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637231068762</td>\n",
       "      <td>æ°‘ä¸» ä½•æ™‚ åˆ° ä¾† ï¼Ÿ</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:40:28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>æ°‘ä¸»ä½•æ™‚åˆ°ä¾†ï¼Ÿ</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.9119</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>243224</td>\n",
       "      <td>206157</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157635548868762_10157635664488762</td>\n",
       "      <td>æ„Ÿè¬ ä¸­è¯ æ°‘åœ‹ ç¸½çµ± ä»—ç¾© åŸ·è¨€ ï¼Œ é¦™æ¸¯ å¿…å‹ ï¼Œ å…±åŒª å¿…äº¡ ã€‚</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>11:51:27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>æ„Ÿè¬ä¸­è¯æ°‘åœ‹ç¸½çµ±ä»—ç¾©åŸ·è¨€ï¼Œ é¦™æ¸¯å¿…å‹ï¼Œå…±åŒªå¿…äº¡ã€‚</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9440</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  parent_id  level object_type   query_status  \\\n",
       "0  243203     206156      2        data  fetched (200)   \n",
       "1  243204     206156      2        data  fetched (200)   \n",
       "2  243206     206156      2        data  fetched (200)   \n",
       "3  243221     206156      2        data  fetched (200)   \n",
       "4  243224     206157      2        data  fetched (200)   \n",
       "\n",
       "                             object_id  \\\n",
       "0  10157637152528762_10157637201198762   \n",
       "1  10157637152528762_10157637240313762   \n",
       "2  10157637152528762_10157637244948762   \n",
       "3  10157637152528762_10157637231068762   \n",
       "4  10157635548868762_10157635664488762   \n",
       "\n",
       "                                             message        date      time  \\\n",
       "0                      é€‰æ‹© ç«çƒ§ ç™½é‡‘æ±‰å®« ï¼Œ æŠ¥åœ† æ˜å›­ è¢« çƒ§ ä¹‹ ä»‡ ï¼   2019/10/01  23:25:11   \n",
       "1  å¦‚æœ ä¸­å›½ æ°‘ä¸» äº† ï¼Œ ä¸ çŸ¥é“ ä¸­å…± ä¼š ä¸ ä¼š ä¸‹å° ï¼Œ å¯ åœ†æ˜å›­ ä¹‹ ä»‡ å¿…æŠ¥ ï¼...  2019/10/01  23:46:48   \n",
       "2  ä»¥å‰ ä»¥ä¸º bbc æ˜¯ ä¸­ç«‹ åª’ä½“ ï¼Œ åŸæ¥ bbc æ”¿æ²» ç«‹åœº æ˜¯ é‚£ä¹ˆ çš„ åæ¿€ ï¼ å°¤...  2019/10/01  23:49:48   \n",
       "3                                       æ°‘ä¸» ä½•æ™‚ åˆ° ä¾† ï¼Ÿ   2019/10/01  23:40:28   \n",
       "4               æ„Ÿè¬ ä¸­è¯ æ°‘åœ‹ ç¸½çµ± ä»—ç¾© åŸ·è¨€ ï¼Œ é¦™æ¸¯ å¿…å‹ ï¼Œ å…±åŒª å¿…äº¡ ã€‚   2019/10/01  11:51:27   \n",
       "\n",
       "   like_count  ...  attachments.data.0.description  attachment.type group  \\\n",
       "0         7.0  ...                             NaN              NaN   bbc   \n",
       "1        13.0  ...                             NaN              NaN   bbc   \n",
       "2         6.0  ...                             NaN              NaN   bbc   \n",
       "3         0.0  ...                             NaN              NaN   bbc   \n",
       "4         2.0  ...                             NaN              NaN   bbc   \n",
       "\n",
       "  prediction                                               text  \\\n",
       "0          0                                 é€‰æ‹©ç«çƒ§ç™½é‡‘æ±‰å®«ï¼ŒæŠ¥åœ†æ˜å›­è¢«çƒ§ä¹‹ä»‡ï¼   \n",
       "1          0      å¦‚æœä¸­å›½æ°‘ä¸»äº†ï¼Œä¸çŸ¥é“ä¸­å…±ä¼šä¸ä¼šä¸‹å°ï¼Œå¯åœ†æ˜å›­ä¹‹ä»‡å¿…æŠ¥ï¼å°æ¹¾å¿…å®šæ­¦ç»Ÿè°¢è°¢ï¼è¿™å°±æ˜¯ä¸­å›½æ°‘æ„ï¼   \n",
       "2          0  ä»¥å‰ä»¥ä¸ºbbcæ˜¯ä¸­ç«‹åª’ä½“ï¼ŒåŸæ¥bbcæ”¿æ²»ç«‹åœºæ˜¯é‚£ä¹ˆçš„åæ¿€ï¼å°¤å…¶æ˜¨å¤©é¦™æ¸¯è­¦å¯Ÿå¼€æªç‰‡æ®µç«Ÿç„¶å‰ªæ‰æš´...   \n",
       "3          0                                            æ°‘ä¸»ä½•æ™‚åˆ°ä¾†ï¼Ÿ   \n",
       "4          0                           æ„Ÿè¬ä¸­è¯æ°‘åœ‹ç¸½çµ±ä»—ç¾©åŸ·è¨€ï¼Œ é¦™æ¸¯å¿…å‹ï¼Œå…±åŒªå¿…äº¡ã€‚   \n",
       "\n",
       "  sentiment_label  sentiment_key positive_probs  negative_probs  simplified  \n",
       "0               1       positive         0.6947          0.3053        True  \n",
       "1               0       negative         0.1343          0.8657        True  \n",
       "2               0       negative         0.0162          0.9838        True  \n",
       "3               0       negative         0.0881          0.9119       False  \n",
       "4               1       positive         0.9440          0.0560       False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Expe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentiment data generated in another file \n",
    "Expe_sentiment = pd.read_csv(\n",
    "r'C:\\R\\SOSC4300\\sentiment930.csv',\n",
    "encoding='UTF8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also check snownlp package, but the result is not as good as Baidu Senta\n",
    "from snownlp import SnowNLP\n",
    "s =  SnowNLP(Fandom['content'].astype(str)[2599])\n",
    "s.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the simplified Chinese deection result \n",
    "Expe = Expe.join(simplify['simplified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>level</th>\n",
       "      <th>object_type</th>\n",
       "      <th>query_status</th>\n",
       "      <th>object_id</th>\n",
       "      <th>message</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>like_count</th>\n",
       "      <th>...</th>\n",
       "      <th>attachments.data.0.description</th>\n",
       "      <th>attachment.type</th>\n",
       "      <th>group</th>\n",
       "      <th>prediction</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_key</th>\n",
       "      <th>positive_probs</th>\n",
       "      <th>negative_probs</th>\n",
       "      <th>simplified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243203</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637201198762</td>\n",
       "      <td>é€‰æ‹© ç«çƒ§ ç™½é‡‘æ±‰å®« ï¼Œ æŠ¥åœ† æ˜å›­ è¢« çƒ§ ä¹‹ ä»‡ ï¼</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:25:11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>é€‰æ‹©ç«çƒ§ç™½é‡‘æ±‰å®«ï¼ŒæŠ¥åœ†æ˜å›­è¢«çƒ§ä¹‹ä»‡ï¼</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6947</td>\n",
       "      <td>0.3053</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>243204</td>\n",
       "      <td>206156</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>fetched (200)</td>\n",
       "      <td>10157637152528762_10157637240313762</td>\n",
       "      <td>å¦‚æœ ä¸­å›½ æ°‘ä¸» äº† ï¼Œ ä¸ çŸ¥é“ ä¸­å…± ä¼š ä¸ ä¼š ä¸‹å° ï¼Œ å¯ åœ†æ˜å›­ ä¹‹ ä»‡ å¿…æŠ¥ ï¼...</td>\n",
       "      <td>2019/10/01</td>\n",
       "      <td>23:46:48</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc</td>\n",
       "      <td>0</td>\n",
       "      <td>å¦‚æœä¸­å›½æ°‘ä¸»äº†ï¼Œä¸çŸ¥é“ä¸­å…±ä¼šä¸ä¼šä¸‹å°ï¼Œå¯åœ†æ˜å›­ä¹‹ä»‡å¿…æŠ¥ï¼å°æ¹¾å¿…å®šæ­¦ç»Ÿè°¢è°¢ï¼è¿™å°±æ˜¯ä¸­å›½æ°‘æ„ï¼</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  parent_id  level object_type   query_status  \\\n",
       "0  243203     206156      2        data  fetched (200)   \n",
       "1  243204     206156      2        data  fetched (200)   \n",
       "\n",
       "                             object_id  \\\n",
       "0  10157637152528762_10157637201198762   \n",
       "1  10157637152528762_10157637240313762   \n",
       "\n",
       "                                             message        date      time  \\\n",
       "0                      é€‰æ‹© ç«çƒ§ ç™½é‡‘æ±‰å®« ï¼Œ æŠ¥åœ† æ˜å›­ è¢« çƒ§ ä¹‹ ä»‡ ï¼   2019/10/01  23:25:11   \n",
       "1  å¦‚æœ ä¸­å›½ æ°‘ä¸» äº† ï¼Œ ä¸ çŸ¥é“ ä¸­å…± ä¼š ä¸ ä¼š ä¸‹å° ï¼Œ å¯ åœ†æ˜å›­ ä¹‹ ä»‡ å¿…æŠ¥ ï¼...  2019/10/01  23:46:48   \n",
       "\n",
       "   like_count  ...  attachments.data.0.description  attachment.type group  \\\n",
       "0         7.0  ...                             NaN              NaN   bbc   \n",
       "1        13.0  ...                             NaN              NaN   bbc   \n",
       "\n",
       "  prediction                                           text sentiment_label  \\\n",
       "0          0                             é€‰æ‹©ç«çƒ§ç™½é‡‘æ±‰å®«ï¼ŒæŠ¥åœ†æ˜å›­è¢«çƒ§ä¹‹ä»‡ï¼               1   \n",
       "1          0  å¦‚æœä¸­å›½æ°‘ä¸»äº†ï¼Œä¸çŸ¥é“ä¸­å…±ä¼šä¸ä¼šä¸‹å°ï¼Œå¯åœ†æ˜å›­ä¹‹ä»‡å¿…æŠ¥ï¼å°æ¹¾å¿…å®šæ­¦ç»Ÿè°¢è°¢ï¼è¿™å°±æ˜¯ä¸­å›½æ°‘æ„ï¼               0   \n",
       "\n",
       "   sentiment_key positive_probs  negative_probs  simplified  \n",
       "0       positive         0.6947          0.3053        True  \n",
       "1       negative         0.1343          0.8657        True  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Expe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed data\n",
    "Expe.to_csv('seventh_expedition_expedition_data_prediction_sentiment.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4123, 21)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide level 2 and level 3 comments\n",
    "Expe_level3= Expe[Expe['level']==3 ]\n",
    "Expe_level3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12646, 21)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Expe_level2= Expe[Expe['level']==2]\n",
    "Expe_level2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3884, 21)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only simplified level 2 comments\n",
    "Expe_level2= Expe[Expe['simplified']==True]\n",
    "Expe_level2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the parent data to the level 3 comments(its children)\n",
    "Expe_merge = pd.merge(Expe_level3,Expe_level2, left_on = 'parent_id',right_on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(955, 42)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Expe_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe the data\n",
    "Expe_merge.to_csv('seventh_expedition_expedition_data_prediction_sentiment_matched.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. Chen, T., & Guestrin, C. (2016). XGBoost. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. https://doi.org/10.1145/2939672.2939785.  \n",
    "\n",
    "2. Fabian Pedregosa, GaÃ«l Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Ã‰douard Duchesnay (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830. \n",
    "\n",
    "3. Ruixuan Luo, Jingjing Xu, Yi Zhang, Xuancheng Ren, Xu Sun. PKUSEG: A Toolkit for Multi-Domain Chinese Word Segmentation. Arxiv. 2019. \n",
    "\n",
    "4. Tian, H., Gao, C., Xiao, X., Liu, H., He, B., Wu, H., Wang, H., & Wu, F. (2020). SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.374. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
